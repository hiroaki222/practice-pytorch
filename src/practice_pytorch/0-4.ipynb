{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using mps\n"
     ]
    }
   ],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'using {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class:tensor([4], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f'Predicted class:{y_pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Flaten()`を使って2次元（28x28）の画像を1次元の784ピクセルの値へと変換する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "flat_image = nn.Flatten()(input_image)\n",
    "print(input_image.size())\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Linear()`で線形変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "hidden1 = nn.Linear(in_features=28*28, out_features=20)(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLUは非線形な活性化関数．  \n",
    "<div style=\"display:flex; align-items: center;\">\n",
    "  <div style=\"flex: 1;\">\n",
    "  <p>シグモイド関数を重ねれば勾配が消える．じゃあ最初から消したれみたいな</p>\n",
    "  <p>計算式がシンプルなので処理が速い</p>\n",
    "  <p>0以下は常に0となるので，ニューロン群の活性化がスパース（sparse： 疎、スカスカ）になり，</p>\n",
    "  <p>発火しないニューロン（＝生体ニューロンに近い動作）も表現できることで精度が向上しやすい</p>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding-left: 20px;\">\n",
    "    <img src=\"https://image.itmedia.co.jp/ait/articles/2003/11/di-01.gif\" width=\"250\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before tensor([[ 0.2961, -0.3432,  0.2652, -0.0196,  0.2460, -0.5234, -0.5450,  0.2542,\n",
      "          0.0142,  0.0186, -0.1364,  0.0399,  0.0216, -0.2731, -0.3349, -0.8027,\n",
      "          0.2520,  0.2269, -0.2311,  0.0732],\n",
      "        [ 0.3415, -0.4126,  0.1674,  0.4506,  0.2617, -0.2493, -0.4176, -0.0111,\n",
      "          0.0323,  0.3014, -0.4279, -0.0962,  0.5515, -0.2296, -0.4217, -0.5802,\n",
      "         -0.1880,  0.1947, -0.2631,  0.1670],\n",
      "        [ 0.7010, -0.0086,  0.1213,  0.2129,  0.0844, -0.5514, -0.4018,  0.0335,\n",
      "          0.1849,  0.1352, -0.4026, -0.2766,  0.1033, -0.5516, -0.1342, -0.5763,\n",
      "         -0.1305, -0.0310,  0.0945, -0.2719]], grad_fn=<AddmmBackward0>)\n",
      "after tensor([[0.2961, 0.0000, 0.2652, 0.0000, 0.2460, 0.0000, 0.0000, 0.2542, 0.0142,\n",
      "         0.0186, 0.0000, 0.0399, 0.0216, 0.0000, 0.0000, 0.0000, 0.2520, 0.2269,\n",
      "         0.0000, 0.0732],\n",
      "        [0.3415, 0.0000, 0.1674, 0.4506, 0.2617, 0.0000, 0.0000, 0.0000, 0.0323,\n",
      "         0.3014, 0.0000, 0.0000, 0.5515, 0.0000, 0.0000, 0.0000, 0.0000, 0.1947,\n",
      "         0.0000, 0.1670],\n",
      "        [0.7010, 0.0000, 0.1213, 0.2129, 0.0844, 0.0000, 0.0000, 0.0335, 0.1849,\n",
      "         0.1352, 0.0000, 0.0000, 0.1033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0945, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'before {hidden1}')\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f'after {hidden1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1034,  0.2871,  0.3943,  0.4723,  0.0888,  0.2177,  0.2782,  0.1377,\n",
      "          0.0326, -0.2032],\n",
      "        [-0.0363,  0.2397,  0.4561,  0.4131,  0.1181,  0.2405,  0.3116,  0.2673,\n",
      "          0.1039, -0.2300],\n",
      "        [ 0.1198,  0.4006,  0.4683,  0.4197,  0.2439,  0.1305,  0.4501,  0.3588,\n",
      "         -0.0066, -0.1807]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=28*28, out_features=20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0910, 0.1094, 0.1218, 0.1316, 0.0897, 0.1021, 0.1084, 0.0942, 0.0848,\n",
      "         0.0670],\n",
      "        [0.0784, 0.1033, 0.1283, 0.1229, 0.0915, 0.1034, 0.1110, 0.1062, 0.0902,\n",
      "         0.0646],\n",
      "        [0.0868, 0.1150, 0.1230, 0.1172, 0.0983, 0.0878, 0.1208, 0.1103, 0.0765,\n",
      "         0.0643]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "print(pred_probab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0303, -0.0210, -0.0154,  ..., -0.0033,  0.0335, -0.0282],\n",
      "        [-0.0005,  0.0172, -0.0178,  ..., -0.0356,  0.0021,  0.0185]],\n",
      "       device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0086, 0.0253], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0190, -0.0187, -0.0085,  ...,  0.0244, -0.0148, -0.0072],\n",
      "        [ 0.0343, -0.0280, -0.0087,  ..., -0.0198, -0.0171, -0.0398]],\n",
      "       device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0381, -0.0232], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0020, -0.0102, -0.0203,  ..., -0.0316,  0.0240,  0.0212],\n",
      "        [-0.0102, -0.0202, -0.0122,  ...,  0.0140, -0.0026,  0.0221]],\n",
      "       device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0200, 0.0338], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
